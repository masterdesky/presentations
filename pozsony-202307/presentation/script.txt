
My name is Balázs Pál, I'm PhD student at the Eötvös Loránd University studying physics, and I do many things that's related to computational sciences. I'm also an assistant research fellow at the Wigner Research Centre for Physics.

In the past couple months I had the opportunity to work with a research group at the Johns Hopkins University, the PFS Galactic Archaeology Team, which is part of the international PFS collaboration, led by the Kavli IPMU: Institute for the Physics and Mathematics of the Universe in Japan. This collaboration is primarily focused on doing research with a new instrument of the Japanese Subaru Telescope at Hawaii, called the Prime Focus Spectrograph (or PFS for short). I'll tell you more about this later.

The collaboration is involved in a vast ocean of research projects all around the world. Today however, in this presentation, I would like to give you a general overview of the science done by the Galactic Archaeology team. Then I would like to talk about my project in it and give you some arguments why it could worth a look for other people too.

================
LAPOZZLAPOZZLAPOZZLAPOZZLAPOZZLAPOZZLAPOZZLAPOZZLAPOZZLAPOZZLAPOZZLAPOZZ
LAPOZZLAPOZZLAPOZZLAPOZZLAPOZZLAPOZZLAPOZZLAPOZZLAPOZZLAPOZZLAPOZZLAPOZZ
LAPOZZLAPOZZLAPOZZLAPOZZLAPOZZLAPOZZLAPOZZLAPOZZLAPOZZLAPOZZLAPOZZLAPOZZ
================

So, first let me tell you the story of what is the motivation, the scientific rationale behind everything the PFS Galactic Archaeology team is doing?

Similarly to particle physics, cosmology has its own standard model. And the concept and goals are the same. While the Standard Model of particle physics intends to explain physical phenomena on the microscopic level, the standard model of cosmology tries to describe the relevant physical phenomena on cosmological scales. More specifically it gives a comprehensive framework for all the phenomena related to the structure and the evolution of the universe. This model is currently called as the "Lambda-CDM model". But the problem is that it is much less established, than the Standard Model of particle physics.

Why is that? In the past couple decades many different problems of this model arised that questioned its accuracy in a multitude of topics and even the model's validity in general. There is a long list of problems that concerns the fundamental tenets and assumptions of the model.

So there are definitely physical models that seen better days than the Lambda-CDM nowadays. Despite all of this, it's still considered to be the standard model of cosmology. However its problems are indisputably widespread and needs to be addressed.

Unfortunately, it's not entirely straightforward how to address all these problems at once. Cosmology is a huge and diverse field in physics, intertwined with other fields like particle physics. So we have quite a large amount of different ways for how we can or how we should address the various challanges that the LCDM has to face.

In the PFS Galactic Archaeology group I had the chance to work on one of these myriads of approaches.

================
LAPOZZLAPOZZLAPOZZLAPOZZLAPOZZLAPOZZLAPOZZLAPOZZLAPOZZLAPOZZLAPOZZLAPOZZ
LAPOZZLAPOZZLAPOZZLAPOZZLAPOZZLAPOZZLAPOZZLAPOZZLAPOZZLAPOZZLAPOZZLAPOZZ
LAPOZZLAPOZZLAPOZZLAPOZZLAPOZZLAPOZZLAPOZZLAPOZZLAPOZZLAPOZZLAPOZZLAPOZZ
================

So let's talk abot this. "Galactic Archaeology": what is it? Well... first of all, I think it is a funny name. When I hear the word "archaeology", I think of ancient cities, and the name "galactic archaeology" sounds even more exotic if I follow this train of thougth. But it actually describes really well, what it's all about!

All the past events and processes that shaped a galaxy at some point in its lifetime, left marks on the stellar attributes of stars inside that galaxy. And by attributes I mean like the chemical composition, spatial distribution, or the dynamics of stars and other things like that. Using these clues imprinted on stars, we can infer how a galaxy evolved in the past couple billion years, we can infer the processes that shaped it. With a better understanding of galactic evolution, we can constrain the nature of the large scale, the cosmological effects that influence every galactic evolution in the universe in the first place.

Just to mention one of these large scale effects, for example the dark matter distribution and the properties of dark matter that - as we currently know or think - inherently drived the clumping of ordinary matter in the young universe, and thus created the first stars, galaxies and galaxy clusters. Later it formed dark matter halos around galaxies and larger structures that we can observe today. This is one of the most important connection that science wants to explore here.

Unfortunately we have lots of unanswered questions about these effects and about most of the related phenomena. However, by successfully constraining all these phenomena, we'll be another step closer to an accurate and valid cosmological standard model. This is the main and long-term motivation of every related research.

================
LAPOZZLAPOZZLAPOZZLAPOZZLAPOZZLAPOZZLAPOZZLAPOZZLAPOZZLAPOZZLAPOZZLAPOZZ
LAPOZZLAPOZZLAPOZZLAPOZZLAPOZZLAPOZZLAPOZZLAPOZZLAPOZZLAPOZZLAPOZZLAPOZZ
LAPOZZLAPOZZLAPOZZLAPOZZLAPOZZLAPOZZLAPOZZLAPOZZLAPOZZLAPOZZLAPOZZLAPOZZ
================

The actual work of the PFS GA group is a bit more nuanced, than what I told you in this little story.

As I mentioned, what is done in galactic archaeology is that we observe things on the small scale to acquire knowledge about the larger scales.

In this running project, the GA team intends to observe mainly bright stars in the dwarf spheroidal galaxies around the Milky Way. Basically in galaxies in the Local Group. And the team wants to extract the relevant physical attributes from these objects. These are targeted, because these smaller galaxies, far away from the center of mass of the dark matter halo, are excellent laboratories to study the properties of the dark matter halo of the Milky Way and the Local Group.

Currently, the best tool we have, and I mean humanity has at its disposal to accurately measure most of the relevant stellar attributes is spectroscopy. And the PFS collaboration obviously wants to carry out a lot of spectroscopical measurement with the PFS instrument on the Subaru Telescope.

Now this instrument is still not in active use, and it will produce data, starting from 2024, hopefully.

================
LAPOZZLAPOZZLAPOZZLAPOZZLAPOZZLAPOZZLAPOZZLAPOZZLAPOZZLAPOZZLAPOZZLAPOZZ
LAPOZZLAPOZZLAPOZZLAPOZZLAPOZZLAPOZZLAPOZZLAPOZZLAPOZZLAPOZZLAPOZZLAPOZZ
LAPOZZLAPOZZLAPOZZLAPOZZLAPOZZLAPOZZLAPOZZLAPOZZLAPOZZLAPOZZLAPOZZLAPOZZ
================

Until then, we are working with synthetic stellar spectra, to develop the pipelines that will be used to analyze the real measurements, once they're ready.

Now, how accurately you can simulate stellar spectra is an another topic on it own. There are several different, elaborate stellar models in existence, but - similarly to the LCDM - they're not really satisfactory. But I won't talk about this. The only thing I'll mention is that based on the existing models, we can generate artificial measurements/simulations that are satisfactory enough for the science we wanted to explore.

And on this slide you can see what these synthetic measurements or simulations of stellar spectra look like. You can notice exactly two things on this figure:
    - The first thing is that measurements are noisy. In both panels, the gray lines are the noisy spectra, while the red and white lines are the underlying, noiseless spectra. It is something that just happens and we have to deal with it in all different kinds of measurements.
    - The second thing is that by doing simulation we have the luxury to generate the underlying, pure stellar spectrum and the noise separately. And soon this will be a very important remark.

================
LAPOZZLAPOZZLAPOZZLAPOZZLAPOZZLAPOZZLAPOZZLAPOZZLAPOZZLAPOZZLAPOZZLAPOZZ
LAPOZZLAPOZZLAPOZZLAPOZZLAPOZZLAPOZZLAPOZZLAPOZZLAPOZZLAPOZZLAPOZZLAPOZZ
LAPOZZLAPOZZLAPOZZLAPOZZLAPOZZLAPOZZLAPOZZLAPOZZLAPOZZLAPOZZLAPOZZLAPOZZ
================

Okay, but what is the reason anyone would care about this in particular and how it's even related to the PFS measurements?

Well, it turns out that with data like this, you can do some extremely useful things that could help not just the entire collaboration, but very much probably everyone, using data of similar nature.

And this is where my project comes in. Probably everybody knows that AI and machine learning is on the rise yet again (or more like still on the rise for many many years now)... And we wanted to test, whether if artificial neural networks could help us answer different questions about stellar spectra. Spoiler: yes, it very much can do that, and it actually does it much better, than expected!

------------------

There are many different questions that can be explored here, but we wanted to focus only on two of them:
    1. Can you substract the noise from the noisy measurements and get back the original, underlying stellar spectra?
    2. Can a neural network learn - as I mentioned - the yet so elusive stellar physics that determines how stellar spectra look like?

I talked a couple words about the second one already. It's unfortunate we don't have a decent stellar model. So it would be great, if a neural network could learn the physical reality without any specific input from us.

However, the goal of denoising data is more elaborate. First of all, it would be obviously great if an AI could just remove the noise from our data on the flip of a switch. Data without noise is almost always preferable. But there are two exact reasons for why we want to do that in the PFS research:
    1. The actual, existing methods for the determination of stellar parameters are simply do not work, when they're presented with a noisy spectrum with the noise counterpart being too large. The reason is that these methods usually involve the substraction of the blackbody continuum from the measurements to get back simply the continuumless absorption lines. However, finding this continuum is many times impossible in noisy conditions. If we could efficiently and accurately denoise the spectra before this step, then we could use the already existing methods to continue the analysis for otherwise unusable data.
    2. An other important issue is that the PFS Galactic Archaeology team has a large list of objets they want to observe. However many of these objects have a high probability of being something else they expect of what that object really is. But if we could tell on-the-fly that "sorry, the object that the telescope's currently targeting is not a red supergiant and you should start observing the next object from the list", that would save the collaboration a lot of observation time and thus a lot of money.

================
LAPOZZLAPOZZLAPOZZLAPOZZLAPOZZLAPOZZLAPOZZLAPOZZLAPOZZLAPOZZLAPOZZLAPOZZ
LAPOZZLAPOZZLAPOZZLAPOZZLAPOZZLAPOZZLAPOZZLAPOZZLAPOZZLAPOZZLAPOZZLAPOZZ
LAPOZZLAPOZZLAPOZZLAPOZZLAPOZZLAPOZZLAPOZZLAPOZZLAPOZZLAPOZZLAPOZZLAPOZZ
================

It was obvious to us at this point that we have to use some form of autoencoder network, a deep neural network architecture with an vast amount of use-scases nowadays. I won't talk about the architectural details today.

This artificial neural network has two sides: and Encoder and a Decoder side.

Its so-called Encoder side basically performs a dimensionality reduction: it encodes a high dimensional data to some low dimensional representation, called latent space. The Decoder side on the right then tries to reconstruct some target data (usually, but not always the original input) from this low dimensional embedding.

And this is exactly what we want! We want to input a noisy spectrum into some neural network and then reconstruct a noiseless version of it on its output.

================
LAPOZZLAPOZZLAPOZZLAPOZZLAPOZZLAPOZZLAPOZZLAPOZZLAPOZZLAPOZZLAPOZZLAPOZZ
LAPOZZLAPOZZLAPOZZLAPOZZLAPOZZLAPOZZLAPOZZLAPOZZLAPOZZLAPOZZLAPOZZLAPOZZ
LAPOZZLAPOZZLAPOZZLAPOZZLAPOZZLAPOZZLAPOZZLAPOZZLAPOZZLAPOZZLAPOZZLAPOZZ
================

After building a model and training it on synthetic data, we concluded that it worked much better than expected.

You can see these results on this slide. The center panel shows a randomly selected spectrum and its reconstructed counterpart by a trained autoencoder on the same plot. They're almost completely overlap, you can't really see the differences between them here on this panel.

That's why I have the bottom panel, which shows the relative errors of the denoising for each sample points, for each pixels. What you can see on this figure is that the hardest part for a neural network to learn is the exact size of the absorption lines in the spectrum. The absorption peaks in the spectrum are where the errors are the largest.

================
LAPOZZLAPOZZLAPOZZLAPOZZLAPOZZLAPOZZLAPOZZLAPOZZLAPOZZLAPOZZLAPOZZLAPOZZ
LAPOZZLAPOZZLAPOZZLAPOZZLAPOZZLAPOZZLAPOZZLAPOZZLAPOZZLAPOZZLAPOZZLAPOZZ
LAPOZZLAPOZZLAPOZZLAPOZZLAPOZZLAPOZZLAPOZZLAPOZZLAPOZZLAPOZZLAPOZZLAPOZZ
================

However there comes the question: is this model just some technical hocus pocus or is this a physically informed and reliable tool for analysis of high resolution spectra?

Rephrasing the question: Is this AI model learns how to denoise the dataset pixel-by-pixel, or does it have any actual idea about physics that is uses during the denoising process?

The test we came up with for this was that we randomly injected physically completely meaningless, but believable, fake "spectral lines" to the original, noiseless spectra. After that we regenerated the noise on these modified spectra and asked an already trained neural network to denoise these modified, noisy spectra.

If this denoising happens pixel-by-pixel and the neural network only learns some form of noise statistics, and how the noise works, then these injected spectral lines will simply survive this process. However if the neural network have any form of idea about the underlying physical processes too, then it will realize that these not lines. They're are lies. Just trickery. Or maybe just noise. Either way, it will think that there are no spectra in existence, where spectral lines appear at these positions, so these lines can be completely ignored.

================
LAPOZZLAPOZZLAPOZZLAPOZZLAPOZZLAPOZZLAPOZZLAPOZZLAPOZZLAPOZZLAPOZZLAPOZZ
LAPOZZLAPOZZLAPOZZLAPOZZLAPOZZLAPOZZLAPOZZLAPOZZLAPOZZLAPOZZLAPOZZLAPOZZ
LAPOZZLAPOZZLAPOZZLAPOZZLAPOZZLAPOZZLAPOZZLAPOZZLAPOZZLAPOZZLAPOZZLAPOZZ
================

That was the idea and that's exactly what we were seeing. Randomly added lines to an existing spectra were always completely ignored.

Again, the top panel shows the noiseless, pure spectrum in blue, which completely disappears under the reconstructed spectrum, drawn in orange. In red, you can see some randomly added, fake spectral lines.

The center panel shows the relative difference between the prediction and the original spectrum. These errors are again, relatively quite low.

In the bottom panel you can see the absolute difference between the prediction of the neural network given on the noisy, but original spectrum and the prediction given on the noisy, but artificially enriched spectrum. There's basically no real difference between them beside noise.

================
LAPOZZLAPOZZLAPOZZLAPOZZLAPOZZLAPOZZLAPOZZLAPOZZLAPOZZLAPOZZLAPOZZLAPOZZ
LAPOZZLAPOZZLAPOZZLAPOZZLAPOZZLAPOZZLAPOZZLAPOZZLAPOZZLAPOZZLAPOZZLAPOZZ
LAPOZZLAPOZZLAPOZZLAPOZZLAPOZZLAPOZZLAPOZZLAPOZZLAPOZZLAPOZZLAPOZZLAPOZZ
================

Is it possible to improve this work? Yes. There are several ways.

Besides endelessly improving the neural network we used, we can eg. ask, if we can build a real denoiser that can recognize and denoise those types of spectra too that it never seen before. Spectra from stars on an entirely different stellar parameter domain could have spectral lines that the model never seen before. Robustly denoising these spectra would be a great achievement.

An interesting problem that we haven't explored yet is the continuum normalization. As I mentioned, several classical methods in the determination of spectral parameters rely on the substraction of the blackbody continuum from the measurements. We can improve our neural networks to the next level by not just denoising measured spectra, but immediately substracting the continuum from them.

Another use-case is to directly train a neural network for the on-the-fly target recognition, again, helping the collaboration to save a lot of observation time.

=================

Time will tell how this knowledge will be used in practice, when the instrument is ready. But that's the end of my presentation, and thank you for yor attention.
